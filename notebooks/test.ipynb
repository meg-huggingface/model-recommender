{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing some model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the logger for the specific package you want to debug\n",
    "import logging\n",
    "\n",
    "\n",
    "package_logger = logging.getLogger('recommender')\n",
    "\n",
    "# Set the logging level of this specific logger to DEBUG\n",
    "package_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommender.main import get_tgi_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 4000, 'max_total_tokens': 4096, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 6144}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=6144, max_input_length=3072, max_total_tokens=4096, num_gpus=1, quantization_type='gptq', estimated_memory_in_gigabytes=20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_id = \"TheBloke/starcoder-GPTQ\"\n",
    "gpu_memory = 24\n",
    "\n",
    "get_tgi_config(model_id,gpu_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-6.7B` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 16384, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 16384, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TGIConfig(model_id='m-a-p/OpenCodeInterpreter-DS-6.7B', max_batch_prefill_tokens=8192, max_input_length=4096, max_total_tokens=8192, num_gpus=1, quantization_type=None, estimated_memory_in_gigabytes=25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tgi_config(\"m-a-p/OpenCodeInterpreter-DS-6.7B\",40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-6.7B` from `transformers`...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MemoryObject(in_bytes=13485223936.0, dtype='float16')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recommender.utils.calcuation import get_model_size\n",
    "\n",
    "get_model_size(\"m-a-p/OpenCodeInterpreter-DS-6.7B\",\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryObject(in_bytes=4395630592, dtype='float16')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recommender.utils.calcuation import get_tgi_memory\n",
    "\n",
    "\n",
    "get_tgi_memory(\n",
    "  model_id=\"m-a-p/OpenCodeInterpreter-DS-6.7B\",\n",
    "  dtype=\"float16\",\n",
    "  max_input_length=4000,\n",
    "  max_prefill_tokens=8192,\n",
    "  max_total_tokens=4096\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 4000, 'max_total_tokens': 4096, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 6144}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 4096}\n",
      "Trying config: {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 4096}\n",
      "Trying config: {'max_input_length': 1512, 'max_total_tokens': 2048, 'max_prefill_tokens': 2048}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 4000, 'max_total_tokens': 4096, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 6144}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=2048, max_input_length=1512, max_total_tokens=2048, num_gpus=1, quantization_type='gptq', estimated_memory_in_gigabytes=15),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=6144, max_input_length=3072, max_total_tokens=4096, num_gpus=1, quantization_type='gptq', estimated_memory_in_gigabytes=20),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=8192, max_input_length=4096, max_total_tokens=8192, num_gpus=1, quantization_type='gptq', estimated_memory_in_gigabytes=32),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=8192, max_input_length=4096, max_total_tokens=8192, num_gpus=2, quantization_type='gptq', estimated_memory_in_gigabytes=33),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=16384, max_input_length=4096, max_total_tokens=8192, num_gpus=1, quantization_type='gptq', estimated_memory_in_gigabytes=53),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=16384, max_input_length=4096, max_total_tokens=8192, num_gpus=4, quantization_type='gptq', estimated_memory_in_gigabytes=57),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=32768, max_input_length=8000, max_total_tokens=8192, num_gpus=2, quantization_type='gptq', estimated_memory_in_gigabytes=130),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=32768, max_input_length=8000, max_total_tokens=8192, num_gpus=8, quantization_type='gptq', estimated_memory_in_gigabytes=137),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=32768, max_input_length=8000, max_total_tokens=8192, num_gpus=4, quantization_type='gptq', estimated_memory_in_gigabytes=133),\n",
       " TGIConfig(model_id='TheBloke/starcoder-GPTQ', max_batch_prefill_tokens=32768, max_input_length=8000, max_total_tokens=8192, num_gpus=8, quantization_type='gptq', estimated_memory_in_gigabytes=137)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recommender.main import get_recommendation\n",
    "\n",
    "\n",
    "get_recommendation(\"TheBloke/starcoder-GPTQ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
