{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing some model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the logger for the specific package you want to debug\n",
    "import logging\n",
    "\n",
    "\n",
    "package_logger = logging.getLogger('recommender')\n",
    "\n",
    "# Set the logging level of this specific logger to DEBUG\n",
    "package_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/hf/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/hf/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from recommender.main import get_tgi_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Filtered configs: [{'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}, {'max_input_length': 4000, 'max_total_tokens': 4096, 'max_prefill_tokens': 32768}, {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}, {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 16384}, {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}, {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}, {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}, {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 6144}, {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 4096}, {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 4096}, {'max_input_length': 1512, 'max_total_tokens': 2048, 'max_prefill_tokens': 2048}]\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4000, 'max_total_tokens': 4096, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 6144}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_id': 'TheBloke/starcoder-GPTQ',\n",
       " 'max_batch_prefill_tokens': 6144,\n",
       " 'max_input_length': 3072,\n",
       " 'max_total_tokens': 4096,\n",
       " 'num_gpus': 1,\n",
       " 'quantization_type': 'gptq',\n",
       " 'estimated_memory_in_gigabytes': 20}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_id = \"TheBloke/starcoder-GPTQ\"\n",
    "gpu_memory = 24\n",
    "\n",
    "get_tgi_config(model_id,gpu_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-6.7B` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 16384, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 16384, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_id': 'm-a-p/OpenCodeInterpreter-DS-6.7B',\n",
       " 'max_batch_prefill_tokens': 8192,\n",
       " 'max_input_length': 4096,\n",
       " 'max_total_tokens': 8192,\n",
       " 'num_gpus': 1,\n",
       " 'quantization_type': None,\n",
       " 'estimated_memory_in_gigabytes': 25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tgi_config(\"m-a-p/OpenCodeInterpreter-DS-6.7B\",40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-6.7B` from `transformers`...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dtype': 'float16',\n",
       " 'model_size_in_bytes': 13485223936.0,\n",
       " 'model_size_in_gigabytes': 12}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recommender.utils.calcuation import get_model_size\n",
    "\n",
    "get_model_size(\"m-a-p/OpenCodeInterpreter-DS-6.7B\",\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /m-a-p/OpenCodeInterpreter-DS-6.7B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:recommender.utils.calcuation:Required memory for prefill: 4294967296\n",
      "DEBUG:recommender.utils.calcuation:Required memory for max total tokens: 25165824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dtype': 'float16', 'memory_in_bytes': 4320133120, 'memory_in_gigabytes': 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recommender.utils.calcuation import get_tgi_memory\n",
    "\n",
    "\n",
    "get_tgi_memory(\n",
    "  model_id=\"m-a-p/OpenCodeInterpreter-DS-6.7B\",\n",
    "  dtype=\"float16\",\n",
    "  max_input_length=4000,\n",
    "  max_prefill_tokens=8192,\n",
    "  max_total_tokens=4096\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "getattr(torch, \"float16\").itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /m-a-p/OpenCodeInterpreter-DS-6.7B/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4192.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig \n",
    "\n",
    "config = AutoConfig.from_pretrained(\"m-a-p/OpenCodeInterpreter-DS-6.7B\")\n",
    "\n",
    "(8192 - 4000) * config.hidden_size * config.num_attention_heads /1024/1024 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/hf/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/hf/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "tokenizer_config.json: 100%|██████████| 677/677 [00:00<00:00, 266kB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 1.92MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 3.81MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 532/532 [00:00<00:00, 1.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 4000, 'max_total_tokens': 4096, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 6144}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 4096}\n",
      "Trying config: {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 4096}\n",
      "Trying config: {'max_input_length': 1512, 'max_total_tokens': 2048, 'max_prefill_tokens': 2048}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 4000, 'max_total_tokens': 4096, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 2048, 'max_total_tokens': 4096, 'max_prefill_tokens': 8192}\n",
      "Trying config: {'max_input_length': 3072, 'max_total_tokens': 4096, 'max_prefill_tokens': 6144}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 8192}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Trying config: {'max_input_length': 4096, 'max_total_tokens': 8192, 'max_prefill_tokens': 16384}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n",
      "Loading pretrained config for `TheBloke/starcoder-GPTQ` from `transformers`...\n",
      "Trying config: {'max_input_length': 8000, 'max_total_tokens': 8192, 'max_prefill_tokens': 32768}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 2048,\n",
       "  'max_input_length': 1512,\n",
       "  'max_total_tokens': 2048,\n",
       "  'num_gpus': 1,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 15},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 6144,\n",
       "  'max_input_length': 3072,\n",
       "  'max_total_tokens': 4096,\n",
       "  'num_gpus': 1,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 20},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 8192,\n",
       "  'max_input_length': 4096,\n",
       "  'max_total_tokens': 8192,\n",
       "  'num_gpus': 1,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 32},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 8192,\n",
       "  'max_input_length': 4096,\n",
       "  'max_total_tokens': 8192,\n",
       "  'num_gpus': 2,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 33},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 16384,\n",
       "  'max_input_length': 4096,\n",
       "  'max_total_tokens': 8192,\n",
       "  'num_gpus': 1,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 53},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 16384,\n",
       "  'max_input_length': 4096,\n",
       "  'max_total_tokens': 8192,\n",
       "  'num_gpus': 4,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 57},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 32768,\n",
       "  'max_input_length': 8000,\n",
       "  'max_total_tokens': 8192,\n",
       "  'num_gpus': 2,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 130},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 32768,\n",
       "  'max_input_length': 8000,\n",
       "  'max_total_tokens': 8192,\n",
       "  'num_gpus': 8,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 137},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 32768,\n",
       "  'max_input_length': 8000,\n",
       "  'max_total_tokens': 8192,\n",
       "  'num_gpus': 4,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 133},\n",
       " {'model_id': 'TheBloke/starcoder-GPTQ',\n",
       "  'max_batch_prefill_tokens': 32768,\n",
       "  'max_input_length': 8000,\n",
       "  'max_total_tokens': 8192,\n",
       "  'num_gpus': 8,\n",
       "  'quantization_type': 'gptq',\n",
       "  'estimated_memory_in_gigabytes': 137}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recommender.main import get_recommendation\n",
    "\n",
    "\n",
    "get_recommendation(\"TheBloke/starcoder-GPTQ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
